{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "dNbPZoPRMtZG",
    "outputId": "55a99027-ed5d-4c07-e18a-8c9a7cb6a3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: torchvision in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg\n",
      "Requirement already satisfied: numpy in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages (from torchvision)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages (from torchvision)\n",
      "Requirement already satisfied: six in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages (from torchvision)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZ-uzulsNlAZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilv5sT0SMvlc"
   },
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "egQMPeiIW3yw"
   },
   "outputs": [],
   "source": [
    "# Default parameters from the PyTorch MNIST example (https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "params = dict(batch_size=64, batch_size_eval=1000, num_epochs=30, learning_rate=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data and make a noisy variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "s-oXQ8R-NeLO",
    "outputId": "4c98b3b1-4b2e-4bd1-a4ed-8c43310c383c"
   },
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "\n",
    "raw_train_dataset = datasets.MNIST('data', train=True, download=True, transform=image_transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, download=True, transform=image_transform)\n",
    "\n",
    "# Split 5k samples from the train dataset for validation (similar to Sacramento et al. 2018).\n",
    "# TODO: Maybe seed this? Cannot be done in the method directly, so would need to manually call torch.seed. \n",
    "train_dataset, val_dataset = torch.utils.data.dataset.random_split(raw_train_dataset, (55000, 5000))\n",
    "\n",
    "kwargs = {'num_workers': 3, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, **kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=params['batch_size_eval'], shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params['batch_size_eval'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0, std=64, scaling_factor=0.5):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.scaling_factor = scaling_factor\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        img_array = np.asarray(img)\n",
    "        noisy_img_array = img_array + self.scaling_factor * np.random.normal(self.mean, self.std, img_array.shape)\n",
    "        noisy_img_array = np.clip(noisy_img_array, 0, 255)\n",
    "        noisy_img_array = noisy_img_array.astype(img_array.dtype)\n",
    "        return Image.fromarray(noisy_img_array)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={}, std={}, scaling_factor={})'.format(self.mean, self.std, self.intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "                       AddGaussianNoise(scaling_factor=2),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "\n",
    "noisy_test_dataset = datasets.MNIST('data', train=False, download=True, transform=image_transform)\n",
    "noisy_test_loader = torch.utils.data.DataLoader(noisy_test_dataset, batch_size=params['batch_size_eval'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(noisy_test_dataset[4][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76ldQAc_ZC-R"
   },
   "outputs": [],
   "source": [
    "class FeedforwardNet(nn.Module):\n",
    "    # TODO: Add ability to easily add more hidden layers. \n",
    "    def __init__(self, num_hidden=500, dropout=0.5):\n",
    "        super(FeedforwardNet, self).__init__()\n",
    "        self.forward1 = nn.Linear(784, num_hidden)\n",
    "        self.forward2 = nn.Linear(num_hidden, 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.forward1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.forward2(x)\n",
    "        # TODO: Maybe change to loss function that includes softmax. \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zU42_5arflnv"
   },
   "outputs": [],
   "source": [
    "class FeedbackNet(nn.Module):\n",
    "    def __init__(self, num_hidden=500, dropout=0.5):\n",
    "        super(FeedbackNet, self).__init__()\n",
    "        self.forward1 = nn.Linear(784, num_hidden)\n",
    "        self.forward2 = nn.Linear(num_hidden, 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.feedback1 = nn.Linear(num_hidden, 784)\n",
    "        self.feedback2 = nn.Linear(10, num_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(-1, 784)\n",
    "        \n",
    "        x = F.relu(self.forward1(inp))\n",
    "        feedback_activation_1 = self.feedback1(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.forward2(x), dim=1)\n",
    "        feedback_activation_2 = self.feedback2(x)\n",
    "        # TODO: Maybe change to loss function that includes softmax. \n",
    "        \n",
    "        #print(feedback_activation_1.shape, feedback_activation_2.shape)\n",
    "        \n",
    "        # 2nd forward pass, this time with added feedback\n",
    "        x = F.relu(self.forward1(inp + feedback_activation_1))\n",
    "        # TODO: Do dropout again here or only on the first pass?\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = F.log_softmax(self.forward2(x + feedback_activation_2), dim=1)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUF21w7MTB8g"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"ConvNet from the official PyTorch tutorial, achieves around 98 % accuracy on test set (https://github.com/pytorch/examples/blob/master/mnist/main.py).\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwwmjtpAaEEN"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, params=None):\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # TODO: Change to loss_function = ..., loss_function(output, target)\n",
    "        #print(output.shape, target.shape)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch * len(data), len(train_loader.dataset),\n",
    "                100. * batch / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR8Tp0QwaFpj"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, params=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # TODO: Change to loss_function = ..., loss_function(output, target)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in filter(lambda p: p.requires_grad, model.parameters()) if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "colab_type": "code",
    "id": "ClDIIE_wRaa0",
    "outputId": "f62ac75f-be70-4b6d-de99-b0f7d9a86d9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 795794 trainable parameters\n",
      "Epoch 1/30\n",
      "Train Epoch: 1 [0/55000 (0%)]\tLoss: 2.385131\n",
      "Train Epoch: 1 [6400/55000 (12%)]\tLoss: 0.431386\n",
      "Train Epoch: 1 [12800/55000 (23%)]\tLoss: 0.464762\n",
      "Train Epoch: 1 [19200/55000 (35%)]\tLoss: 0.108397\n",
      "Train Epoch: 1 [25600/55000 (47%)]\tLoss: 0.262161\n",
      "Train Epoch: 1 [32000/55000 (58%)]\tLoss: 0.277559\n",
      "Train Epoch: 1 [38400/55000 (70%)]\tLoss: 0.294778\n",
      "Train Epoch: 1 [44800/55000 (81%)]\tLoss: 0.150399\n",
      "Train Epoch: 1 [51200/55000 (93%)]\tLoss: 0.125782\n",
      "Took 11.702157258987427 seconds\n",
      "\n",
      "Test set: Average loss: 0.1754, Accuracy: 9480/10000 (95%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.2840, Accuracy: 5518/10000 (55%)\n",
      "\n",
      "Epoch 2/30\n",
      "Train Epoch: 2 [0/55000 (0%)]\tLoss: 0.135305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-58c026162ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Took'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-bf5789005fce>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, params)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2445\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2340\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2341\u001b[0m     \"\"\"\n\u001b[1;32m   2342\u001b[0m     \u001b[0mCreates\u001b[0m \u001b[0man\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0mreferencing\u001b[0m \u001b[0mpixel\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbyte\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#model = FeedforwardNet(num_hidden=500).to(device)\n",
    "model = FeedbackNet().to(device)\n",
    "print('Model has', count_parameters(model), 'trainable parameters')\n",
    "\n",
    "# TODO: Maybe use Adam or Adadelta instead. \n",
    "optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=params['momentum'])\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    print('Epoch {}/{}'.format(epoch+1, params['num_epochs']))\n",
    "    start_time = time.time()\n",
    "    train(model, device, train_loader, optimizer, epoch+1)\n",
    "    print('Took', time.time()-start_time, 'seconds')\n",
    "    test(model, device, test_loader)\n",
    "    print('Now testing on noisy dataset...')\n",
    "    test(model, device, noisy_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Sequential Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(batch_size=64, batch_size_eval=1000, num_epochs=30, learning_rate=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset and arrange digits in sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Refactor this to datasets.py.\n",
    "image_transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "\n",
    "raw_train_dataset = datasets.MNIST('data', train=True, download=True, transform=image_transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, download=True, transform=image_transform)\n",
    "\n",
    "# Split 5k samples from the train dataset for validation (similar to Sacramento et al. 2018).\n",
    "# TODO: Maybe seed this? Cannot be done in the method directly, so would need to manually call torch.seed. \n",
    "train_dataset, val_dataset = torch.utils.data.dataset.random_split(raw_train_dataset, (55000, 5000))\n",
    "\n",
    "kwargs = {'num_workers': 3, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, **kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=params['batch_size_eval'], shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params['batch_size_eval'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UhweJ8OePN6"
   },
   "outputs": [],
   "source": [
    "num_allowed_seqs = 100\n",
    "seq_len = 5\n",
    "\n",
    "allowed_seqs = np.random.randint(0, 10, (num_allowed_seqs, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYYIKUvWdyZl"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ImageSequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, allowed_seqs, image_dataset, num_classes, num_samples=10000):\n",
    "        self.num_samples = num_samples\n",
    "        self.allowed_seqs = allowed_seqs\n",
    "        self.images_per_class = {i: [] for i in range(num_classes)}\n",
    "        for image, class_ in train_dataset:\n",
    "            self.images_per_class[class_.item()].append(image)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        seq = self.allowed_seqs[i % len(self.allowed_seqs)]\n",
    "        images = [random.choice(self.images_per_class[class_]) for class_ in seq]\n",
    "        return torch.cat(images), seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muWGKNxql2lO"
   },
   "outputs": [],
   "source": [
    "def plot_image_sequence(images, targets):\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(1, len(images), i+1)\n",
    "        plt.imshow(images[i], cmap='Greys')\n",
    "        plt.axis('off')\n",
    "        plt.title(targets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8woSGMdwlP4k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_train_dataset = ImageSequenceDataset(allowed_seqs, train_dataset, 10, num_samples=50000)\n",
    "seq_test_dataset = ImageSequenceDataset(allowed_seqs, test_dataset, 10, num_samples=10000)\n",
    "\n",
    "seq_train_loader = torch.utils.data.DataLoader(seq_train_dataset, batch_size=params['batch_size'], shuffle=True, **kwargs)\n",
    "seq_test_loader = torch.utils.data.DataLoader(seq_test_dataset, batch_size=params['batch_size_eval'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rDykJGxbwq2m",
    "outputId": "e23d5b48-0025-4e21-80d0-7f3f46ff84a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28, 28])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "A_2w2yEJmCLH",
    "outputId": "31c8cab6-ef7a-46fc-b861-8af892c51763"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image_sequence(*seq_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image_sequence(*seq_train_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialLSTM(nn.Module):\n",
    "    def __init__(self, num_hidden=500):\n",
    "        super(SequentialLSTM, self).__init__()\n",
    "        # TODO: Maybe change everything to batch_second.\n",
    "        self.lstm1 = nn.LSTM(784, num_hidden, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(num_hidden, 10, batch_first=True)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(inp.shape[0], inp.shape[1], 784)\n",
    "        #print(inp.shape)\n",
    "        x, (hidden, cell) = self.lstm1(inp)\n",
    "        # TODO: Need to add a non-linearity here?\n",
    "        x, (hidden, cell) = self.lstm2(x)\n",
    "        #print(x.shape, hidden.shape, cell.shape)\n",
    "        #print(x.shape)\n",
    "        x = F.log_softmax(x, dim=2)\n",
    "        #print(x.sum(dim=(2)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialFeedforwardNet(nn.Module):\n",
    "    def __init__(self, num_hidden=500, dropout=0.5):\n",
    "        super(SequentialFeedforwardNet, self).__init__()\n",
    "        self.forward1 = nn.Linear(784, num_hidden)\n",
    "        self.forward2 = nn.Linear(num_hidden, 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        outp = torch.zeros(inp.shape[0], inp.shape[1], 10)\n",
    "        \n",
    "        for i, x in enumerate(inp.transpose(0, 1)):\n",
    "            x = x.view(-1, 784)\n",
    "            x = F.relu(self.forward1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.forward2(x)\n",
    "            # TODO: Maybe change to loss function that includes softmax. \n",
    "            x = F.log_softmax(x, dim=1)\n",
    "            outp[:, i] = x\n",
    "            \n",
    "        return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialFeedbackNet(nn.Module):\n",
    "    def __init__(self, num_hidden=500, dropout=0.5, alpha=0.5):\n",
    "        super(SequentialFeedbackNet, self).__init__()\n",
    "        self.forward1 = nn.Linear(784, num_hidden)\n",
    "        self.forward2 = nn.Linear(num_hidden, 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.feedback1 = nn.Linear(num_hidden, 784)\n",
    "        self.feedback2 = nn.Linear(10, num_hidden)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.no_feedback = False\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        outp = torch.zeros(inp.shape[0], inp.shape[1], 10)\n",
    "        \n",
    "        feedback_activation_1 = torch.zeros(inp.shape[0], self.forward1.weight.shape[1])\n",
    "        feedback_activation_2 = torch.zeros(inp.shape[0], self.forward2.weight.shape[1])\n",
    "        \n",
    "        for i, x in enumerate(inp.transpose(0, 1)):\n",
    "            x = x.view(-1, 784)\n",
    "            x = F.relu(self.forward1((1 - self.alpha) * x + self.alpha * feedback_activation_1))\n",
    "            # TODO: Is this retained across forward passes?\n",
    "            feedback_activation_1 = self.feedback1(x)  # for next forward pass\n",
    "            x = self.dropout(x)\n",
    "            x = self.forward2((1 - self.alpha) * x +  self.alpha * feedback_activation_2)\n",
    "            # TODO: Maybe change to loss function that includes softmax. \n",
    "            x = F.log_softmax(x, dim=1)\n",
    "            feedback_activation_2 = self.feedback2(x)  # for next forward pass\n",
    "            outp[:, i] = x\n",
    "            \n",
    "        return outp\n",
    "\n",
    "    def forward_old(self, inp):\n",
    "        # inp shape: batch_size, time_steps, 28, 28\n",
    "        # TODO: Maybe change to batch_second. \n",
    "        output = torch.zeros(inp.shape[0], inp.shape[1], 10)  # shape: batch_size, time_steps, 10\n",
    "        \n",
    "        for i, x in enumerate(inp.transpose(0, 1)):  # iterate over time dimension\n",
    "            #print(x.shape)\n",
    "            \n",
    "            x = x.view(-1, 784)  # shape: batch_size, 784\n",
    "            # TODO: Implement multiple feedback passes. \n",
    "            \n",
    "            if i == 0 or self.no_feedback:\n",
    "                x = self.forward1(x)\n",
    "            else:\n",
    "                print(feedback_activation_1.shape)\n",
    "                #plt.imshow(feedback_activation_1.detach()[0].view(28, 28))\n",
    "                x = self.forward1(x + self.alpha * feedback_activation_1)\n",
    "            x = F.relu(x)\n",
    "            if not self.no_feedback:\n",
    "                feedback_activation_1 = self.feedback1(x)\n",
    "\n",
    "            x = self.dropout(x)\n",
    "            if i == 0 or self.no_feedback:\n",
    "                x = self.forward2(x)\n",
    "            else:\n",
    "                x = self.forward2(x + self.alpha * feedback_activation_2)\n",
    "            x = F.log_softmax(x, dim=1)\n",
    "            if not self.no_feedback:\n",
    "                feedback_activation_2 = self.feedback2(x)\n",
    "\n",
    "            output[:, i] = x\n",
    "    \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ff_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7af4b45350b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_train_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialFeedbackNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ff_model' is not defined"
     ]
    }
   ],
   "source": [
    "imgs = seq_train_dataset[0][0]\n",
    "model = SequentialFeedbackNet()\n",
    "model.forward1.weight.data = ff_model.forward1.weight.data.clone()\n",
    "model.forward1.bias.data = ff_model.forward1.bias.data.clone()\n",
    "model.forward2.weight.data = ff_model.forward2.weight.data.clone()\n",
    "model.forward2.bias.data = ff_model.forward2.bias.data.clone()\n",
    "model(imgs[None], no_feedback=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x125515400>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good template for hyperparameter optimization:\n",
    "#def run(train_dataset, val_dataset, device, params, history)\n",
    "#def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Debug properly.\n",
    "def train_sequential(model, device, train_loader, optimizer, epoch, params=None):\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(seq_train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        #print(output.shape, target.shape)\n",
    "        \n",
    "        # TODO: According to https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "        #       need to do model.zero_grad() and model.hidden = model.init_hidden() here. \n",
    "\n",
    "        loss = F.nll_loss(output.transpose(1, 2), target)  # loss function requires transpose\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch * len(data), len(train_loader.dataset),\n",
    "                100. * batch / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Debug properly.\n",
    "def test_sequential(model, device, test_loader, params=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output.transpose(1, 2), target, reduction='sum').item() # sum up batch loss, loss function requires transpose\n",
    "            \n",
    "            pred = output.view(-1, output.shape[-1]).max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # TODO: Make this independent of seq_len (used twice!).\n",
    "    test_loss /= len(test_loader.dataset) * seq_len\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset) * seq_len,\n",
    "        100. * correct / (len(test_loader.dataset) * seq_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c975f60cfd64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "len([p for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.348487\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.029409\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.685362\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.685626\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.854768\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.842595\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.857510\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.559042\n",
      "Took 17.25441312789917 seconds\n",
      "\n",
      "Test set: Average loss: 0.2941, Accuracy: 46396/50000 (93%)\n",
      "\n",
      "Epoch 2/30\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.291564\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.406237\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.417039\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.386074\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.438656\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.373355\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.247864\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.281522\n",
      "Took 18.256911039352417 seconds\n",
      "\n",
      "Test set: Average loss: 0.1679, Accuracy: 47960/50000 (96%)\n",
      "\n",
      "Epoch 3/30\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.242734\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.456790\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.182501\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.249323\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.234820\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.199779\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.198553\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.239245\n",
      "Took 17.39103078842163 seconds\n",
      "\n",
      "Test set: Average loss: 0.1263, Accuracy: 48587/50000 (97%)\n",
      "\n",
      "Epoch 4/30\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.289842\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.235673\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.199045\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.209527\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.172664\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.278044\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.292348\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.275580\n",
      "Took 21.45059084892273 seconds\n",
      "\n",
      "Test set: Average loss: 0.1132, Accuracy: 48710/50000 (97%)\n",
      "\n",
      "Epoch 5/30\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.310791\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.233899\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.215827\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.171799\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.180940\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.228010\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.199415\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.214544\n",
      "Took 17.09770703315735 seconds\n",
      "\n",
      "Test set: Average loss: 0.0943, Accuracy: 48937/50000 (98%)\n",
      "\n",
      "Epoch 6/30\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.448676\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.353092\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.193744\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.196547\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.230494\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.260886\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.198064\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.328493\n",
      "Took 18.738765001296997 seconds\n",
      "\n",
      "Test set: Average loss: 0.0829, Accuracy: 48957/50000 (98%)\n",
      "\n",
      "Epoch 7/30\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.147383\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.259330\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.248601\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.163918\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.265021\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.193157\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.361643\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.230280\n",
      "Took 17.885653018951416 seconds\n",
      "\n",
      "Test set: Average loss: 0.0830, Accuracy: 49036/50000 (98%)\n",
      "\n",
      "Epoch 8/30\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.230612\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.247160\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.237025\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.381503\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.468715\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.328461\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.275273\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.237573\n",
      "Took 17.256153106689453 seconds\n",
      "\n",
      "Test set: Average loss: 0.0754, Accuracy: 49069/50000 (98%)\n",
      "\n",
      "Epoch 9/30\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.481165\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.290300\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.449842\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.182708\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.277421\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.264493\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.385700\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.262717\n",
      "Took 17.26965880393982 seconds\n",
      "\n",
      "Test set: Average loss: 0.0785, Accuracy: 49124/50000 (98%)\n",
      "\n",
      "Epoch 10/30\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.267391\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.289798\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.337250\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.297638\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.230789\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.264656\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.334657\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.365773\n",
      "Took 17.834930896759033 seconds\n",
      "\n",
      "Test set: Average loss: 0.0806, Accuracy: 49091/50000 (98%)\n",
      "\n",
      "Epoch 11/30\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.384856\n",
      "Train Epoch: 11 [6400/50000 (13%)]\tLoss: 0.274241\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 0.142099\n",
      "Train Epoch: 11 [19200/50000 (38%)]\tLoss: 0.287363\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.314451\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 0.298007\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 0.311264\n",
      "Train Epoch: 11 [44800/50000 (90%)]\tLoss: 0.163401\n",
      "Took 22.471959829330444 seconds\n",
      "\n",
      "Test set: Average loss: 0.0801, Accuracy: 49117/50000 (98%)\n",
      "\n",
      "Epoch 12/30\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.375388\n",
      "Train Epoch: 12 [6400/50000 (13%)]\tLoss: 0.420930\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 0.301339\n",
      "Train Epoch: 12 [19200/50000 (38%)]\tLoss: 0.212433\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.447974\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 0.210150\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 0.330730\n",
      "Train Epoch: 12 [44800/50000 (90%)]\tLoss: 0.275360\n",
      "Took 18.244712829589844 seconds\n",
      "\n",
      "Test set: Average loss: 0.0905, Accuracy: 49072/50000 (98%)\n",
      "\n",
      "Epoch 13/30\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.538656\n",
      "Train Epoch: 13 [6400/50000 (13%)]\tLoss: 0.396051\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 0.352884\n",
      "Train Epoch: 13 [19200/50000 (38%)]\tLoss: 0.404541\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.295980\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 0.351305\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 0.383338\n",
      "Train Epoch: 13 [44800/50000 (90%)]\tLoss: 0.359212\n",
      "Took 17.648789167404175 seconds\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 48990/50000 (98%)\n",
      "\n",
      "Epoch 14/30\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.235769\n",
      "Train Epoch: 14 [6400/50000 (13%)]\tLoss: 0.436554\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 0.345918\n",
      "Train Epoch: 14 [19200/50000 (38%)]\tLoss: 0.180792\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.539521\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 0.519012\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.262863\n",
      "Train Epoch: 14 [44800/50000 (90%)]\tLoss: 0.318237\n",
      "Took 17.394907236099243 seconds\n",
      "\n",
      "Test set: Average loss: 0.1023, Accuracy: 48938/50000 (98%)\n",
      "\n",
      "Epoch 15/30\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.274520\n",
      "Train Epoch: 15 [6400/50000 (13%)]\tLoss: 0.531485\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 0.368149\n",
      "Train Epoch: 15 [19200/50000 (38%)]\tLoss: 0.407190\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.234721\n",
      "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 0.353758\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.274561\n",
      "Train Epoch: 15 [44800/50000 (90%)]\tLoss: 0.448799\n",
      "Took 17.126829147338867 seconds\n",
      "\n",
      "Test set: Average loss: 0.1114, Accuracy: 48831/50000 (98%)\n",
      "\n",
      "Epoch 16/30\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.285813\n",
      "Train Epoch: 16 [6400/50000 (13%)]\tLoss: 0.400946\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 0.320221\n",
      "Train Epoch: 16 [19200/50000 (38%)]\tLoss: 0.252394\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.302786\n",
      "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 0.346698\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 0.478025\n",
      "Train Epoch: 16 [44800/50000 (90%)]\tLoss: 0.400292\n",
      "Took 17.45693612098694 seconds\n",
      "\n",
      "Test set: Average loss: 0.1085, Accuracy: 48780/50000 (98%)\n",
      "\n",
      "Epoch 17/30\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.541196\n",
      "Train Epoch: 17 [6400/50000 (13%)]\tLoss: 0.699870\n",
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 0.442088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [19200/50000 (38%)]\tLoss: 0.229007\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.681468\n",
      "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 0.806415\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 0.418699\n",
      "Train Epoch: 17 [44800/50000 (90%)]\tLoss: 0.278279\n",
      "Took 17.365716218948364 seconds\n",
      "\n",
      "Test set: Average loss: 0.1225, Accuracy: 48689/50000 (97%)\n",
      "\n",
      "Epoch 18/30\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.426534\n",
      "Train Epoch: 18 [6400/50000 (13%)]\tLoss: 0.292203\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 0.313897\n",
      "Train Epoch: 18 [19200/50000 (38%)]\tLoss: 0.730151\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.279924\n",
      "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 0.480329\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 0.390277\n",
      "Train Epoch: 18 [44800/50000 (90%)]\tLoss: 0.481792\n",
      "Took 17.90710711479187 seconds\n",
      "\n",
      "Test set: Average loss: 0.1345, Accuracy: 48655/50000 (97%)\n",
      "\n",
      "Epoch 19/30\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.741272\n",
      "Train Epoch: 19 [6400/50000 (13%)]\tLoss: 0.588074\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 0.394015\n",
      "Train Epoch: 19 [19200/50000 (38%)]\tLoss: 0.271873\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.316488\n",
      "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 0.367525\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 0.449479\n",
      "Train Epoch: 19 [44800/50000 (90%)]\tLoss: 0.331387\n",
      "Took 17.69953989982605 seconds\n",
      "\n",
      "Test set: Average loss: 0.1376, Accuracy: 48499/50000 (97%)\n",
      "\n",
      "Epoch 20/30\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.575172\n",
      "Train Epoch: 20 [6400/50000 (13%)]\tLoss: 0.275749\n",
      "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 0.599394\n",
      "Train Epoch: 20 [19200/50000 (38%)]\tLoss: 0.345381\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.535465\n",
      "Train Epoch: 20 [32000/50000 (64%)]\tLoss: 0.454022\n",
      "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 0.563296\n",
      "Train Epoch: 20 [44800/50000 (90%)]\tLoss: 0.534215\n",
      "Took 17.33576798439026 seconds\n",
      "\n",
      "Test set: Average loss: 0.1349, Accuracy: 48592/50000 (97%)\n",
      "\n",
      "Epoch 21/30\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.312959\n",
      "Train Epoch: 21 [6400/50000 (13%)]\tLoss: 0.465553\n",
      "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 0.451099\n",
      "Train Epoch: 21 [19200/50000 (38%)]\tLoss: 0.396936\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.332540\n",
      "Train Epoch: 21 [32000/50000 (64%)]\tLoss: 0.516904\n",
      "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 0.367472\n",
      "Train Epoch: 21 [44800/50000 (90%)]\tLoss: 0.411585\n",
      "Took 17.219146013259888 seconds\n",
      "\n",
      "Test set: Average loss: 0.1439, Accuracy: 48415/50000 (97%)\n",
      "\n",
      "Epoch 22/30\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.354188\n",
      "Train Epoch: 22 [6400/50000 (13%)]\tLoss: 0.375322\n",
      "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 0.518847\n",
      "Train Epoch: 22 [19200/50000 (38%)]\tLoss: 0.357139\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.517902\n",
      "Train Epoch: 22 [32000/50000 (64%)]\tLoss: 0.407125\n",
      "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 0.684047\n",
      "Train Epoch: 22 [44800/50000 (90%)]\tLoss: 0.408166\n",
      "Took 17.037029027938843 seconds\n",
      "\n",
      "Test set: Average loss: 0.1534, Accuracy: 48112/50000 (96%)\n",
      "\n",
      "Epoch 23/30\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.302498\n",
      "Train Epoch: 23 [6400/50000 (13%)]\tLoss: 0.552347\n",
      "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 0.466866\n",
      "Train Epoch: 23 [19200/50000 (38%)]\tLoss: 1.018513\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.435660\n",
      "Train Epoch: 23 [32000/50000 (64%)]\tLoss: 0.416631\n",
      "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 0.452685\n",
      "Train Epoch: 23 [44800/50000 (90%)]\tLoss: 1.384544\n",
      "Took 17.03612184524536 seconds\n",
      "\n",
      "Test set: Average loss: 0.1647, Accuracy: 48195/50000 (96%)\n",
      "\n",
      "Epoch 24/30\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.429999\n",
      "Train Epoch: 24 [6400/50000 (13%)]\tLoss: 0.494631\n",
      "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 0.291543\n",
      "Train Epoch: 24 [19200/50000 (38%)]\tLoss: 0.572218\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.418133\n",
      "Train Epoch: 24 [32000/50000 (64%)]\tLoss: 0.717704\n",
      "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 0.892231\n",
      "Train Epoch: 24 [44800/50000 (90%)]\tLoss: 0.510639\n",
      "Took 16.43036389350891 seconds\n",
      "\n",
      "Test set: Average loss: 0.1652, Accuracy: 47976/50000 (96%)\n",
      "\n",
      "Epoch 25/30\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.428235\n",
      "Train Epoch: 25 [6400/50000 (13%)]\tLoss: 0.498976\n",
      "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 0.447207\n",
      "Train Epoch: 25 [19200/50000 (38%)]\tLoss: 0.387912\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.643170\n",
      "Train Epoch: 25 [32000/50000 (64%)]\tLoss: 0.412441\n",
      "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 0.664999\n",
      "Train Epoch: 25 [44800/50000 (90%)]\tLoss: 0.702740\n",
      "Took 16.49971914291382 seconds\n",
      "\n",
      "Test set: Average loss: 0.1787, Accuracy: 47728/50000 (95%)\n",
      "\n",
      "Epoch 26/30\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.594551\n",
      "Train Epoch: 26 [6400/50000 (13%)]\tLoss: 0.455027\n",
      "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 0.573201\n",
      "Train Epoch: 26 [19200/50000 (38%)]\tLoss: 0.350471\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.547919\n",
      "Train Epoch: 26 [32000/50000 (64%)]\tLoss: 0.400324\n",
      "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 0.436719\n",
      "Train Epoch: 26 [44800/50000 (90%)]\tLoss: 0.827527\n",
      "Took 16.42193293571472 seconds\n",
      "\n",
      "Test set: Average loss: 0.1939, Accuracy: 47660/50000 (95%)\n",
      "\n",
      "Epoch 27/30\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.442542\n",
      "Train Epoch: 27 [6400/50000 (13%)]\tLoss: 0.552942\n",
      "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 0.440559\n",
      "Train Epoch: 27 [19200/50000 (38%)]\tLoss: 0.380594\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.427557\n",
      "Train Epoch: 27 [32000/50000 (64%)]\tLoss: 0.456219\n",
      "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 0.542008\n",
      "Train Epoch: 27 [44800/50000 (90%)]\tLoss: 0.560385\n",
      "Took 16.456334114074707 seconds\n",
      "\n",
      "Test set: Average loss: 0.1920, Accuracy: 47571/50000 (95%)\n",
      "\n",
      "Epoch 28/30\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.412927\n",
      "Train Epoch: 28 [6400/50000 (13%)]\tLoss: 0.966628\n",
      "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 0.391345\n",
      "Train Epoch: 28 [19200/50000 (38%)]\tLoss: 0.351924\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.419221\n",
      "Train Epoch: 28 [32000/50000 (64%)]\tLoss: 0.461613\n",
      "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 0.521680\n",
      "Train Epoch: 28 [44800/50000 (90%)]\tLoss: 0.861706\n",
      "Took 16.445003986358643 seconds\n",
      "\n",
      "Test set: Average loss: 0.1992, Accuracy: 47447/50000 (95%)\n",
      "\n",
      "Epoch 29/30\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.547526\n",
      "Train Epoch: 29 [6400/50000 (13%)]\tLoss: 0.485518\n",
      "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 0.414963\n",
      "Train Epoch: 29 [19200/50000 (38%)]\tLoss: 0.708913\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.630507\n",
      "Train Epoch: 29 [32000/50000 (64%)]\tLoss: 0.559988\n",
      "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 0.452467\n",
      "Train Epoch: 29 [44800/50000 (90%)]\tLoss: 0.333022\n",
      "Took 16.469528198242188 seconds\n",
      "\n",
      "Test set: Average loss: 0.2154, Accuracy: 46985/50000 (94%)\n",
      "\n",
      "Epoch 30/30\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.563161\n",
      "Train Epoch: 30 [6400/50000 (13%)]\tLoss: 0.390673\n",
      "Train Epoch: 30 [12800/50000 (26%)]\tLoss: 0.561610\n",
      "Train Epoch: 30 [19200/50000 (38%)]\tLoss: 0.536105\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.384929\n",
      "Train Epoch: 30 [32000/50000 (64%)]\tLoss: 0.475098\n",
      "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 0.581808\n",
      "Train Epoch: 30 [44800/50000 (90%)]\tLoss: 0.685948\n",
      "Took 16.38791584968567 seconds\n",
      "\n",
      "Test set: Average loss: 0.2170, Accuracy: 46777/50000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model = SequentialFeedforwardNet().to(device)\n",
    "#model = SequentialLSTM().to(device)\n",
    "model = SequentialFeedbackNet(dropout=0.5, alpha=0).to(device)\n",
    "# TODO: Use rmsprop or another optimizer which is good for LSTMs.\n",
    "#optimizer = optim.SGD([p for p in model.parameters() if p.requires_grad], lr=0.0001, momentum=params['momentum'])\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=params['momentum'])\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    print('Epoch {}/{}'.format(epoch+1, params['num_epochs']))\n",
    "    start_time = time.time()\n",
    "    train_sequential(model, device, seq_train_loader, optimizer, epoch+1)\n",
    "    print('Took', time.time()-start_time, 'seconds')\n",
    "    \n",
    "    test_sequential(model, device, seq_test_loader)\n",
    "    #print('Now testing on noisy dataset...')\n",
    "    #test(model, device, noisy_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-58-981f29c607c0>\u001b[0m(23)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     21 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m            \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 23 \u001b[0;31m            \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfeedback_activation_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m            \u001b[0;31m# TODO: Is this retained across forward passes?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m            \u001b[0mfeedback_activation_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedback1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# for next forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x.shape\n",
      "torch.Size([64, 784])\n",
      "ipdb> feedback_activation_1.shape\n",
      "torch.Size([64, 500])\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialFeedbackNet(dropout=0, alpha=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward1.weight.data = ff_model.forward1.weight.data.clone()\n",
    "model.forward1.bias.data = ff_model.forward1.bias.data.clone()\n",
    "model.forward2.weight.data = ff_model.forward2.weight.data.clone()\n",
    "model.forward2.bias.data = ff_model.forward2.bias.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5467, Accuracy: 46727/10000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sequential(model, device, seq_test_loader, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward1.weight.requires_grad = False\n",
    "model.forward1.bias.requires_grad = False\n",
    "model.forward2.weight.requires_grad = False\n",
    "model.forward2.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5414, Accuracy: 46792/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1806, Accuracy: 9459/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(ff_model, device, test_loader, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "feedback-connections.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
