{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "dNbPZoPRMtZG",
    "outputId": "55a99027-ed5d-4c07-e18a-8c9a7cb6a3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: torchvision in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg\n",
      "Requirement already satisfied: numpy in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages (from torchvision)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages (from torchvision)\n",
      "Requirement already satisfied: six in /Users/jrieke/anaconda2/envs/py3/lib/python3.6/site-packages (from torchvision)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZ-uzulsNlAZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilv5sT0SMvlc"
   },
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "egQMPeiIW3yw"
   },
   "outputs": [],
   "source": [
    "# Default parameters from the PyTorch MNIST example (https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "params = dict(batch_size=64, batch_size_eval=1000, num_epochs=30, learning_rate=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "s-oXQ8R-NeLO",
    "outputId": "4c98b3b1-4b2e-4bd1-a4ed-8c43310c383c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "image_transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "\n",
    "raw_train_dataset = datasets.MNIST('data', train=True, download=True, transform=image_transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, download=True, transform=image_transform)\n",
    "\n",
    "# Split 5k samples from the train dataset for validation (similar to Sacramento et al. 2018).\n",
    "# TODO: Maybe seed this? Cannot be done in the method directly, so would need to manually call torch.seed. \n",
    "train_dataset, val_dataset = torch.utils.data.dataset.random_split(raw_train_dataset, (55000, 5000))\n",
    "\n",
    "kwargs = {'num_workers': 3, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, **kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=params['batch_size_eval'], shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params['batch_size_eval'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make noisy test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0, std=64, scaling_factor=0.5):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.scaling_factor = scaling_factor\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        img_array = np.asarray(img)\n",
    "        noisy_img_array = img_array + self.scaling_factor * np.random.normal(self.mean, self.std, img_array.shape)\n",
    "        noisy_img_array = np.clip(noisy_img_array, 0, 255)\n",
    "        noisy_img_array = noisy_img_array.astype(img_array.dtype)\n",
    "        return Image.fromarray(noisy_img_array)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={}, std={}, scaling_factor={})'.format(self.mean, self.std, self.intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "                       AddGaussianNoise(scaling_factor=2),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "\n",
    "noisy_test_dataset = datasets.MNIST('data', train=False, download=True, transform=image_transform)\n",
    "noisy_test_loader = torch.utils.data.DataLoader(noisy_test_dataset, batch_size=params['batch_size_eval'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sequential dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UhweJ8OePN6"
   },
   "outputs": [],
   "source": [
    "num_allowed_seqs = 10\n",
    "seq_len = 5\n",
    "\n",
    "allowed_seqs = np.random.randint(0, 10, (num_allowed_seqs, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYYIKUvWdyZl"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ImageSequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, allowed_seqs, image_dataset, num_classes):\n",
    "        self.allowed_seqs = allowed_seqs\n",
    "        self.images_per_class = {i: [] for i in range(num_classes)}\n",
    "        for image, class_ in train_dataset:\n",
    "            self.images_per_class[class_.item()].append(image)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(allowed_seqs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        images = [random.choice(self.images_per_class[class_]) for class_ in allowed_seqs[i]]\n",
    "        return torch.cat(images), allowed_seqs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muWGKNxql2lO"
   },
   "outputs": [],
   "source": [
    "def plot_image_sequence(images, targets):\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(1, len(images), i+1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(targets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8woSGMdwlP4k",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-de2360c60b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageSequenceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallowed_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mseq_test_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageSequenceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallowed_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseq_train_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mseq_test_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_test_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_eval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bef2e4ee5ce5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, allowed_seqs, image_dataset, num_classes)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallowed_seqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_per_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensor is not a torch image.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# TODO: make efficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    426\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq_train_dataset = ImageSequenceDataset(allowed_seqs, train_dataset, 10)\n",
    "seq_test_dataset = ImageSequenceDataset(allowed_seqs, test_dataset, 10)\n",
    "\n",
    "seq_train_loader = torch.utils.data.DataLoader(seq_train_dataset, batch_size=params['batch_size'], shuffle=True, **kwargs)\n",
    "seq_test_loader = torch.utils.data.DataLoader(seq_test_dataset, batch_size=params['batch_size_eval'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rDykJGxbwq2m",
    "outputId": "e23d5b48-0025-4e21-80d0-7f3f46ff84a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28, 28])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "A_2w2yEJmCLH",
    "outputId": "31c8cab6-ef7a-46fc-b861-8af892c51763"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAB7CAYAAAA1+ooUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJF8AADwj+QIA4Nn/AQ4UXVfpzCtPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f050425f588>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image_sequence(*seq_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0MJJfTUFncp3",
    "outputId": "2d2c5dde-0dff-49bc-be19-673d871ecc2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 28, 28]) torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "for data, target in seq_train_loader:\n",
    "    print(data.shape, target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76ldQAc_ZC-R"
   },
   "outputs": [],
   "source": [
    "class FeedforwardNet(nn.Module):\n",
    "    # TODO: Add ability to easily add more hidden layers. \n",
    "    def __init__(self, num_hidden=500, dropout=0.5):\n",
    "        super(FeedforwardNet, self).__init__()\n",
    "        self.forward1 = nn.Linear(784, num_hidden)\n",
    "        self.forward2 = nn.Linear(num_hidden, 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.forward1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.forward2(x)\n",
    "        # TODO: Maybe change to loss function that includes softmax. \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zU42_5arflnv"
   },
   "outputs": [],
   "source": [
    "class FeedbackNet(nn.Module):\n",
    "    def __init__(self, num_hidden=500, dropout=0.5):\n",
    "        super(FeedbackNet, self).__init__()\n",
    "        self.forward1 = nn.Linear(784, num_hidden)\n",
    "        self.forward2 = nn.Linear(num_hidden, 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.feedback1 = nn.Linear(num_hidden, 784)\n",
    "        self.feedback2 = nn.Linear(10, num_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.view(-1, 784)\n",
    "        \n",
    "        x = F.relu(self.forward1(inp))\n",
    "        feedback_activation_1 = self.feedback1(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.forward2(x), dim=1)\n",
    "        feedback_activation_2 = self.feedback2(x)\n",
    "        # TODO: Maybe change to loss function that includes softmax. \n",
    "        \n",
    "        #print(feedback_activation_1.shape, feedback_activation_2.shape)\n",
    "        \n",
    "        # 2nd forward pass, this time with added feedback\n",
    "        x = F.relu(self.forward1(inp + feedback_activation_1))\n",
    "        # TODO: Do dropout again here or only on the first pass?\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = F.log_softmax(self.forward2(x + feedback_activation_2), dim=1)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUF21w7MTB8g"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"ConvNet from the official PyTorch tutorial, achieves around 98 % accuracy on test set (https://github.com/pytorch/examples/blob/master/mnist/main.py).\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwwmjtpAaEEN"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, params=None):\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # TODO: Change to loss_function = ..., loss_function(output, target)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch * len(data), len(train_loader.dataset),\n",
    "                100. * batch / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR8Tp0QwaFpj"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, params=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # TODO: Change to loss_function = ..., loss_function(output, target)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in filter(lambda p: p.requires_grad, model.parameters()) if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "colab_type": "code",
    "id": "ClDIIE_wRaa0",
    "outputId": "f62ac75f-be70-4b6d-de99-b0f7d9a86d9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 795794 trianable parameters\n",
      "Epoch 1/30\n",
      "Train Epoch: 1 [0/55000 (0%)]\tLoss: 2.599647\n",
      "Train Epoch: 1 [6400/55000 (12%)]\tLoss: 0.433526\n",
      "Train Epoch: 1 [12800/55000 (23%)]\tLoss: 0.550520\n",
      "Train Epoch: 1 [19200/55000 (35%)]\tLoss: 0.409129\n",
      "Train Epoch: 1 [25600/55000 (47%)]\tLoss: 0.151310\n",
      "Train Epoch: 1 [32000/55000 (58%)]\tLoss: 0.220261\n",
      "Train Epoch: 1 [38400/55000 (70%)]\tLoss: 0.252231\n",
      "Train Epoch: 1 [44800/55000 (81%)]\tLoss: 0.085708\n",
      "Train Epoch: 1 [51200/55000 (93%)]\tLoss: 0.210743\n",
      "Took 11.529579877853394 seconds\n",
      "\n",
      "Test set: Average loss: 0.1694, Accuracy: 9469/10000 (95%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.9910, Accuracy: 6377/10000 (64%)\n",
      "\n",
      "Epoch 2/30\n",
      "Train Epoch: 2 [0/55000 (0%)]\tLoss: 0.192741\n",
      "Train Epoch: 2 [6400/55000 (12%)]\tLoss: 0.180095\n",
      "Train Epoch: 2 [12800/55000 (23%)]\tLoss: 0.233141\n",
      "Train Epoch: 2 [19200/55000 (35%)]\tLoss: 0.083444\n",
      "Train Epoch: 2 [25600/55000 (47%)]\tLoss: 0.157057\n",
      "Train Epoch: 2 [32000/55000 (58%)]\tLoss: 0.207071\n",
      "Train Epoch: 2 [38400/55000 (70%)]\tLoss: 0.177559\n",
      "Train Epoch: 2 [44800/55000 (81%)]\tLoss: 0.045797\n",
      "Train Epoch: 2 [51200/55000 (93%)]\tLoss: 0.120412\n",
      "Took 12.953229904174805 seconds\n",
      "\n",
      "Test set: Average loss: 0.1228, Accuracy: 9615/10000 (96%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.9833, Accuracy: 6460/10000 (65%)\n",
      "\n",
      "Epoch 3/30\n",
      "Train Epoch: 3 [0/55000 (0%)]\tLoss: 0.102405\n",
      "Train Epoch: 3 [6400/55000 (12%)]\tLoss: 0.143041\n",
      "Train Epoch: 3 [12800/55000 (23%)]\tLoss: 0.093884\n",
      "Train Epoch: 3 [19200/55000 (35%)]\tLoss: 0.167875\n",
      "Train Epoch: 3 [25600/55000 (47%)]\tLoss: 0.080510\n",
      "Train Epoch: 3 [32000/55000 (58%)]\tLoss: 0.104260\n",
      "Train Epoch: 3 [38400/55000 (70%)]\tLoss: 0.060567\n",
      "Train Epoch: 3 [44800/55000 (81%)]\tLoss: 0.146597\n",
      "Train Epoch: 3 [51200/55000 (93%)]\tLoss: 0.113191\n",
      "Took 11.95100998878479 seconds\n",
      "\n",
      "Test set: Average loss: 0.1011, Accuracy: 9683/10000 (97%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.9239, Accuracy: 6743/10000 (67%)\n",
      "\n",
      "Epoch 4/30\n",
      "Train Epoch: 4 [0/55000 (0%)]\tLoss: 0.063809\n",
      "Train Epoch: 4 [6400/55000 (12%)]\tLoss: 0.039212\n",
      "Train Epoch: 4 [12800/55000 (23%)]\tLoss: 0.115111\n",
      "Train Epoch: 4 [19200/55000 (35%)]\tLoss: 0.051590\n",
      "Train Epoch: 4 [25600/55000 (47%)]\tLoss: 0.084562\n",
      "Train Epoch: 4 [32000/55000 (58%)]\tLoss: 0.117479\n",
      "Train Epoch: 4 [38400/55000 (70%)]\tLoss: 0.035289\n",
      "Train Epoch: 4 [44800/55000 (81%)]\tLoss: 0.071096\n",
      "Train Epoch: 4 [51200/55000 (93%)]\tLoss: 0.125685\n",
      "Took 13.06878113746643 seconds\n",
      "\n",
      "Test set: Average loss: 0.0889, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.7496, Accuracy: 7392/10000 (74%)\n",
      "\n",
      "Epoch 5/30\n",
      "Train Epoch: 5 [0/55000 (0%)]\tLoss: 0.071684\n",
      "Train Epoch: 5 [6400/55000 (12%)]\tLoss: 0.062894\n",
      "Train Epoch: 5 [12800/55000 (23%)]\tLoss: 0.106294\n",
      "Train Epoch: 5 [19200/55000 (35%)]\tLoss: 0.170494\n",
      "Train Epoch: 5 [25600/55000 (47%)]\tLoss: 0.035581\n",
      "Train Epoch: 5 [32000/55000 (58%)]\tLoss: 0.061445\n",
      "Train Epoch: 5 [38400/55000 (70%)]\tLoss: 0.034157\n",
      "Train Epoch: 5 [44800/55000 (81%)]\tLoss: 0.086208\n",
      "Train Epoch: 5 [51200/55000 (93%)]\tLoss: 0.111486\n",
      "Took 12.867151975631714 seconds\n",
      "\n",
      "Test set: Average loss: 0.0758, Accuracy: 9773/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.7914, Accuracy: 7304/10000 (73%)\n",
      "\n",
      "Epoch 6/30\n",
      "Train Epoch: 6 [0/55000 (0%)]\tLoss: 0.069846\n",
      "Train Epoch: 6 [6400/55000 (12%)]\tLoss: 0.027921\n",
      "Train Epoch: 6 [12800/55000 (23%)]\tLoss: 0.015515\n",
      "Train Epoch: 6 [19200/55000 (35%)]\tLoss: 0.101018\n",
      "Train Epoch: 6 [25600/55000 (47%)]\tLoss: 0.129799\n",
      "Train Epoch: 6 [32000/55000 (58%)]\tLoss: 0.067247\n",
      "Train Epoch: 6 [38400/55000 (70%)]\tLoss: 0.100741\n",
      "Train Epoch: 6 [44800/55000 (81%)]\tLoss: 0.175533\n",
      "Train Epoch: 6 [51200/55000 (93%)]\tLoss: 0.067139\n",
      "Took 12.493120908737183 seconds\n",
      "\n",
      "Test set: Average loss: 0.0738, Accuracy: 9772/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.8112, Accuracy: 7303/10000 (73%)\n",
      "\n",
      "Epoch 7/30\n",
      "Train Epoch: 7 [0/55000 (0%)]\tLoss: 0.022284\n",
      "Train Epoch: 7 [6400/55000 (12%)]\tLoss: 0.122857\n",
      "Train Epoch: 7 [12800/55000 (23%)]\tLoss: 0.033492\n",
      "Train Epoch: 7 [19200/55000 (35%)]\tLoss: 0.012322\n",
      "Train Epoch: 7 [25600/55000 (47%)]\tLoss: 0.076375\n",
      "Train Epoch: 7 [32000/55000 (58%)]\tLoss: 0.058338\n",
      "Train Epoch: 7 [38400/55000 (70%)]\tLoss: 0.026074\n",
      "Train Epoch: 7 [44800/55000 (81%)]\tLoss: 0.045006\n",
      "Train Epoch: 7 [51200/55000 (93%)]\tLoss: 0.039869\n",
      "Took 12.922716856002808 seconds\n",
      "\n",
      "Test set: Average loss: 0.0682, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.7565, Accuracy: 7468/10000 (75%)\n",
      "\n",
      "Epoch 8/30\n",
      "Train Epoch: 8 [0/55000 (0%)]\tLoss: 0.031186\n",
      "Train Epoch: 8 [6400/55000 (12%)]\tLoss: 0.024806\n",
      "Train Epoch: 8 [12800/55000 (23%)]\tLoss: 0.223341\n",
      "Train Epoch: 8 [19200/55000 (35%)]\tLoss: 0.009249\n",
      "Train Epoch: 8 [25600/55000 (47%)]\tLoss: 0.027017\n",
      "Train Epoch: 8 [32000/55000 (58%)]\tLoss: 0.074952\n",
      "Train Epoch: 8 [38400/55000 (70%)]\tLoss: 0.067660\n",
      "Train Epoch: 8 [44800/55000 (81%)]\tLoss: 0.038658\n",
      "Train Epoch: 8 [51200/55000 (93%)]\tLoss: 0.050212\n",
      "Took 12.270204067230225 seconds\n",
      "\n",
      "Test set: Average loss: 0.0679, Accuracy: 9786/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.8686, Accuracy: 7262/10000 (73%)\n",
      "\n",
      "Epoch 9/30\n",
      "Train Epoch: 9 [0/55000 (0%)]\tLoss: 0.022297\n",
      "Train Epoch: 9 [6400/55000 (12%)]\tLoss: 0.072156\n",
      "Train Epoch: 9 [12800/55000 (23%)]\tLoss: 0.078955\n",
      "Train Epoch: 9 [19200/55000 (35%)]\tLoss: 0.022140\n",
      "Train Epoch: 9 [25600/55000 (47%)]\tLoss: 0.045044\n",
      "Train Epoch: 9 [32000/55000 (58%)]\tLoss: 0.056534\n",
      "Train Epoch: 9 [38400/55000 (70%)]\tLoss: 0.005856\n",
      "Train Epoch: 9 [44800/55000 (81%)]\tLoss: 0.062078\n",
      "Train Epoch: 9 [51200/55000 (93%)]\tLoss: 0.027629\n",
      "Took 11.36456823348999 seconds\n",
      "\n",
      "Test set: Average loss: 0.0685, Accuracy: 9789/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.8913, Accuracy: 7203/10000 (72%)\n",
      "\n",
      "Epoch 10/30\n",
      "Train Epoch: 10 [0/55000 (0%)]\tLoss: 0.051143\n",
      "Train Epoch: 10 [6400/55000 (12%)]\tLoss: 0.019494\n",
      "Train Epoch: 10 [12800/55000 (23%)]\tLoss: 0.028051\n",
      "Train Epoch: 10 [19200/55000 (35%)]\tLoss: 0.051153\n",
      "Train Epoch: 10 [25600/55000 (47%)]\tLoss: 0.018373\n",
      "Train Epoch: 10 [32000/55000 (58%)]\tLoss: 0.041668\n",
      "Train Epoch: 10 [38400/55000 (70%)]\tLoss: 0.015750\n",
      "Train Epoch: 10 [44800/55000 (81%)]\tLoss: 0.007263\n",
      "Train Epoch: 10 [51200/55000 (93%)]\tLoss: 0.023950\n",
      "Took 11.437790155410767 seconds\n",
      "\n",
      "Test set: Average loss: 0.0645, Accuracy: 9811/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.8688, Accuracy: 7387/10000 (74%)\n",
      "\n",
      "Epoch 11/30\n",
      "Train Epoch: 11 [0/55000 (0%)]\tLoss: 0.104108\n",
      "Train Epoch: 11 [6400/55000 (12%)]\tLoss: 0.021465\n",
      "Train Epoch: 11 [12800/55000 (23%)]\tLoss: 0.048203\n",
      "Train Epoch: 11 [19200/55000 (35%)]\tLoss: 0.012988\n",
      "Train Epoch: 11 [25600/55000 (47%)]\tLoss: 0.053054\n",
      "Train Epoch: 11 [32000/55000 (58%)]\tLoss: 0.016426\n",
      "Train Epoch: 11 [38400/55000 (70%)]\tLoss: 0.014655\n",
      "Train Epoch: 11 [44800/55000 (81%)]\tLoss: 0.020699\n",
      "Train Epoch: 11 [51200/55000 (93%)]\tLoss: 0.034631\n",
      "Took 11.377619981765747 seconds\n",
      "\n",
      "Test set: Average loss: 0.0685, Accuracy: 9797/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.7901, Accuracy: 7523/10000 (75%)\n",
      "\n",
      "Epoch 12/30\n",
      "Train Epoch: 12 [0/55000 (0%)]\tLoss: 0.006969\n",
      "Train Epoch: 12 [6400/55000 (12%)]\tLoss: 0.035560\n",
      "Train Epoch: 12 [12800/55000 (23%)]\tLoss: 0.018510\n",
      "Train Epoch: 12 [19200/55000 (35%)]\tLoss: 0.051980\n",
      "Train Epoch: 12 [25600/55000 (47%)]\tLoss: 0.033165\n",
      "Train Epoch: 12 [32000/55000 (58%)]\tLoss: 0.029553\n",
      "Train Epoch: 12 [38400/55000 (70%)]\tLoss: 0.007261\n",
      "Train Epoch: 12 [44800/55000 (81%)]\tLoss: 0.107052\n",
      "Train Epoch: 12 [51200/55000 (93%)]\tLoss: 0.009556\n",
      "Took 14.292964935302734 seconds\n",
      "\n",
      "Test set: Average loss: 0.0639, Accuracy: 9811/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.9556, Accuracy: 7274/10000 (73%)\n",
      "\n",
      "Epoch 13/30\n",
      "Train Epoch: 13 [0/55000 (0%)]\tLoss: 0.005128\n",
      "Train Epoch: 13 [6400/55000 (12%)]\tLoss: 0.055709\n",
      "Train Epoch: 13 [12800/55000 (23%)]\tLoss: 0.028607\n",
      "Train Epoch: 13 [19200/55000 (35%)]\tLoss: 0.016756\n",
      "Train Epoch: 13 [25600/55000 (47%)]\tLoss: 0.024825\n",
      "Train Epoch: 13 [32000/55000 (58%)]\tLoss: 0.003641\n",
      "Train Epoch: 13 [38400/55000 (70%)]\tLoss: 0.020084\n",
      "Train Epoch: 13 [44800/55000 (81%)]\tLoss: 0.005465\n",
      "Train Epoch: 13 [51200/55000 (93%)]\tLoss: 0.014355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 11.483542203903198 seconds\n",
      "\n",
      "Test set: Average loss: 0.0635, Accuracy: 9797/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.9088, Accuracy: 7395/10000 (74%)\n",
      "\n",
      "Epoch 14/30\n",
      "Train Epoch: 14 [0/55000 (0%)]\tLoss: 0.014144\n",
      "Train Epoch: 14 [6400/55000 (12%)]\tLoss: 0.008342\n",
      "Train Epoch: 14 [12800/55000 (23%)]\tLoss: 0.033322\n",
      "Train Epoch: 14 [19200/55000 (35%)]\tLoss: 0.034618\n",
      "Train Epoch: 14 [25600/55000 (47%)]\tLoss: 0.015373\n",
      "Train Epoch: 14 [32000/55000 (58%)]\tLoss: 0.003172\n",
      "Train Epoch: 14 [38400/55000 (70%)]\tLoss: 0.008841\n",
      "Train Epoch: 14 [44800/55000 (81%)]\tLoss: 0.018476\n",
      "Train Epoch: 14 [51200/55000 (93%)]\tLoss: 0.027754\n",
      "Took 11.93544602394104 seconds\n",
      "\n",
      "Test set: Average loss: 0.0645, Accuracy: 9806/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.9777, Accuracy: 7309/10000 (73%)\n",
      "\n",
      "Epoch 15/30\n",
      "Train Epoch: 15 [0/55000 (0%)]\tLoss: 0.006222\n",
      "Train Epoch: 15 [6400/55000 (12%)]\tLoss: 0.058559\n",
      "Train Epoch: 15 [12800/55000 (23%)]\tLoss: 0.005951\n",
      "Train Epoch: 15 [19200/55000 (35%)]\tLoss: 0.006679\n",
      "Train Epoch: 15 [25600/55000 (47%)]\tLoss: 0.079586\n",
      "Train Epoch: 15 [32000/55000 (58%)]\tLoss: 0.005705\n",
      "Train Epoch: 15 [38400/55000 (70%)]\tLoss: 0.004999\n",
      "Train Epoch: 15 [44800/55000 (81%)]\tLoss: 0.096832\n",
      "Train Epoch: 15 [51200/55000 (93%)]\tLoss: 0.013327\n",
      "Took 14.386343240737915 seconds\n",
      "\n",
      "Test set: Average loss: 0.0635, Accuracy: 9813/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.9340, Accuracy: 7408/10000 (74%)\n",
      "\n",
      "Epoch 16/30\n",
      "Train Epoch: 16 [0/55000 (0%)]\tLoss: 0.002344\n",
      "Train Epoch: 16 [6400/55000 (12%)]\tLoss: 0.058326\n",
      "Train Epoch: 16 [12800/55000 (23%)]\tLoss: 0.015817\n",
      "Train Epoch: 16 [19200/55000 (35%)]\tLoss: 0.008759\n",
      "Train Epoch: 16 [25600/55000 (47%)]\tLoss: 0.008801\n",
      "Train Epoch: 16 [32000/55000 (58%)]\tLoss: 0.091239\n",
      "Train Epoch: 16 [38400/55000 (70%)]\tLoss: 0.007122\n",
      "Train Epoch: 16 [44800/55000 (81%)]\tLoss: 0.020003\n",
      "Train Epoch: 16 [51200/55000 (93%)]\tLoss: 0.020183\n",
      "Took 11.561145067214966 seconds\n",
      "\n",
      "Test set: Average loss: 0.0670, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.9918, Accuracy: 7358/10000 (74%)\n",
      "\n",
      "Epoch 17/30\n",
      "Train Epoch: 17 [0/55000 (0%)]\tLoss: 0.006467\n",
      "Train Epoch: 17 [6400/55000 (12%)]\tLoss: 0.010263\n",
      "Train Epoch: 17 [12800/55000 (23%)]\tLoss: 0.017900\n",
      "Train Epoch: 17 [19200/55000 (35%)]\tLoss: 0.004024\n",
      "Train Epoch: 17 [25600/55000 (47%)]\tLoss: 0.020745\n",
      "Train Epoch: 17 [32000/55000 (58%)]\tLoss: 0.016513\n",
      "Train Epoch: 17 [38400/55000 (70%)]\tLoss: 0.088177\n",
      "Train Epoch: 17 [44800/55000 (81%)]\tLoss: 0.007830\n",
      "Train Epoch: 17 [51200/55000 (93%)]\tLoss: 0.039317\n",
      "Took 12.409058094024658 seconds\n",
      "\n",
      "Test set: Average loss: 0.0642, Accuracy: 9818/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 0.9695, Accuracy: 7447/10000 (74%)\n",
      "\n",
      "Epoch 18/30\n",
      "Train Epoch: 18 [0/55000 (0%)]\tLoss: 0.001886\n",
      "Train Epoch: 18 [6400/55000 (12%)]\tLoss: 0.003264\n",
      "Train Epoch: 18 [12800/55000 (23%)]\tLoss: 0.007745\n",
      "Train Epoch: 18 [19200/55000 (35%)]\tLoss: 0.023749\n",
      "Train Epoch: 18 [25600/55000 (47%)]\tLoss: 0.023293\n",
      "Train Epoch: 18 [32000/55000 (58%)]\tLoss: 0.004391\n",
      "Train Epoch: 18 [38400/55000 (70%)]\tLoss: 0.008802\n",
      "Train Epoch: 18 [44800/55000 (81%)]\tLoss: 0.008529\n",
      "Train Epoch: 18 [51200/55000 (93%)]\tLoss: 0.010289\n",
      "Took 13.021193981170654 seconds\n",
      "\n",
      "Test set: Average loss: 0.0636, Accuracy: 9811/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.0572, Accuracy: 7265/10000 (73%)\n",
      "\n",
      "Epoch 19/30\n",
      "Train Epoch: 19 [0/55000 (0%)]\tLoss: 0.023842\n",
      "Train Epoch: 19 [6400/55000 (12%)]\tLoss: 0.032995\n",
      "Train Epoch: 19 [12800/55000 (23%)]\tLoss: 0.008578\n",
      "Train Epoch: 19 [19200/55000 (35%)]\tLoss: 0.007435\n",
      "Train Epoch: 19 [25600/55000 (47%)]\tLoss: 0.002873\n",
      "Train Epoch: 19 [32000/55000 (58%)]\tLoss: 0.007219\n",
      "Train Epoch: 19 [38400/55000 (70%)]\tLoss: 0.001971\n",
      "Train Epoch: 19 [44800/55000 (81%)]\tLoss: 0.007445\n",
      "Train Epoch: 19 [51200/55000 (93%)]\tLoss: 0.009726\n",
      "Took 11.610864162445068 seconds\n",
      "\n",
      "Test set: Average loss: 0.0621, Accuracy: 9808/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.0052, Accuracy: 7422/10000 (74%)\n",
      "\n",
      "Epoch 20/30\n",
      "Train Epoch: 20 [0/55000 (0%)]\tLoss: 0.022794\n",
      "Train Epoch: 20 [6400/55000 (12%)]\tLoss: 0.001223\n",
      "Train Epoch: 20 [12800/55000 (23%)]\tLoss: 0.016839\n",
      "Train Epoch: 20 [19200/55000 (35%)]\tLoss: 0.020023\n",
      "Train Epoch: 20 [25600/55000 (47%)]\tLoss: 0.012645\n",
      "Train Epoch: 20 [32000/55000 (58%)]\tLoss: 0.008862\n",
      "Train Epoch: 20 [38400/55000 (70%)]\tLoss: 0.023107\n",
      "Train Epoch: 20 [44800/55000 (81%)]\tLoss: 0.018048\n",
      "Train Epoch: 20 [51200/55000 (93%)]\tLoss: 0.017560\n",
      "Took 14.628365993499756 seconds\n",
      "\n",
      "Test set: Average loss: 0.0633, Accuracy: 9819/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.1943, Accuracy: 7117/10000 (71%)\n",
      "\n",
      "Epoch 21/30\n",
      "Train Epoch: 21 [0/55000 (0%)]\tLoss: 0.013078\n",
      "Train Epoch: 21 [6400/55000 (12%)]\tLoss: 0.015138\n",
      "Train Epoch: 21 [12800/55000 (23%)]\tLoss: 0.005910\n",
      "Train Epoch: 21 [19200/55000 (35%)]\tLoss: 0.004279\n",
      "Train Epoch: 21 [25600/55000 (47%)]\tLoss: 0.029611\n",
      "Train Epoch: 21 [32000/55000 (58%)]\tLoss: 0.036616\n",
      "Train Epoch: 21 [38400/55000 (70%)]\tLoss: 0.003072\n",
      "Train Epoch: 21 [44800/55000 (81%)]\tLoss: 0.001806\n",
      "Train Epoch: 21 [51200/55000 (93%)]\tLoss: 0.002461\n",
      "Took 12.009407997131348 seconds\n",
      "\n",
      "Test set: Average loss: 0.0646, Accuracy: 9822/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.1760, Accuracy: 7182/10000 (72%)\n",
      "\n",
      "Epoch 22/30\n",
      "Train Epoch: 22 [0/55000 (0%)]\tLoss: 0.004444\n",
      "Train Epoch: 22 [6400/55000 (12%)]\tLoss: 0.004557\n",
      "Train Epoch: 22 [12800/55000 (23%)]\tLoss: 0.003067\n",
      "Train Epoch: 22 [19200/55000 (35%)]\tLoss: 0.007277\n",
      "Train Epoch: 22 [25600/55000 (47%)]\tLoss: 0.007334\n",
      "Train Epoch: 22 [32000/55000 (58%)]\tLoss: 0.007797\n",
      "Train Epoch: 22 [38400/55000 (70%)]\tLoss: 0.008209\n",
      "Train Epoch: 22 [44800/55000 (81%)]\tLoss: 0.012404\n",
      "Train Epoch: 22 [51200/55000 (93%)]\tLoss: 0.007204\n",
      "Took 12.294242143630981 seconds\n",
      "\n",
      "Test set: Average loss: 0.0612, Accuracy: 9823/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.2012, Accuracy: 7168/10000 (72%)\n",
      "\n",
      "Epoch 23/30\n",
      "Train Epoch: 23 [0/55000 (0%)]\tLoss: 0.011141\n",
      "Train Epoch: 23 [6400/55000 (12%)]\tLoss: 0.003994\n",
      "Train Epoch: 23 [12800/55000 (23%)]\tLoss: 0.018030\n",
      "Train Epoch: 23 [19200/55000 (35%)]\tLoss: 0.011846\n",
      "Train Epoch: 23 [25600/55000 (47%)]\tLoss: 0.014338\n",
      "Train Epoch: 23 [32000/55000 (58%)]\tLoss: 0.010524\n",
      "Train Epoch: 23 [38400/55000 (70%)]\tLoss: 0.005134\n",
      "Train Epoch: 23 [44800/55000 (81%)]\tLoss: 0.024398\n",
      "Train Epoch: 23 [51200/55000 (93%)]\tLoss: 0.008363\n",
      "Took 13.894204139709473 seconds\n",
      "\n",
      "Test set: Average loss: 0.0673, Accuracy: 9814/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.1788, Accuracy: 7280/10000 (73%)\n",
      "\n",
      "Epoch 24/30\n",
      "Train Epoch: 24 [0/55000 (0%)]\tLoss: 0.014944\n",
      "Train Epoch: 24 [6400/55000 (12%)]\tLoss: 0.005196\n",
      "Train Epoch: 24 [12800/55000 (23%)]\tLoss: 0.011256\n",
      "Train Epoch: 24 [19200/55000 (35%)]\tLoss: 0.001465\n",
      "Train Epoch: 24 [25600/55000 (47%)]\tLoss: 0.006831\n",
      "Train Epoch: 24 [32000/55000 (58%)]\tLoss: 0.011722\n",
      "Train Epoch: 24 [38400/55000 (70%)]\tLoss: 0.020037\n",
      "Train Epoch: 24 [44800/55000 (81%)]\tLoss: 0.006452\n",
      "Train Epoch: 24 [51200/55000 (93%)]\tLoss: 0.014682\n",
      "Took 13.859857082366943 seconds\n",
      "\n",
      "Test set: Average loss: 0.0659, Accuracy: 9826/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.3226, Accuracy: 7126/10000 (71%)\n",
      "\n",
      "Epoch 25/30\n",
      "Train Epoch: 25 [0/55000 (0%)]\tLoss: 0.002900\n",
      "Train Epoch: 25 [6400/55000 (12%)]\tLoss: 0.036863\n",
      "Train Epoch: 25 [12800/55000 (23%)]\tLoss: 0.012621\n",
      "Train Epoch: 25 [19200/55000 (35%)]\tLoss: 0.013612\n",
      "Train Epoch: 25 [25600/55000 (47%)]\tLoss: 0.005227\n",
      "Train Epoch: 25 [32000/55000 (58%)]\tLoss: 0.001453\n",
      "Train Epoch: 25 [38400/55000 (70%)]\tLoss: 0.010276\n",
      "Train Epoch: 25 [44800/55000 (81%)]\tLoss: 0.000645\n",
      "Train Epoch: 25 [51200/55000 (93%)]\tLoss: 0.006915\n",
      "Took 11.549752950668335 seconds\n",
      "\n",
      "Test set: Average loss: 0.0665, Accuracy: 9820/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.4219, Accuracy: 6926/10000 (69%)\n",
      "\n",
      "Epoch 26/30\n",
      "Train Epoch: 26 [0/55000 (0%)]\tLoss: 0.029721\n",
      "Train Epoch: 26 [6400/55000 (12%)]\tLoss: 0.012206\n",
      "Train Epoch: 26 [12800/55000 (23%)]\tLoss: 0.007636\n",
      "Train Epoch: 26 [19200/55000 (35%)]\tLoss: 0.001218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [25600/55000 (47%)]\tLoss: 0.003082\n",
      "Train Epoch: 26 [32000/55000 (58%)]\tLoss: 0.019367\n",
      "Train Epoch: 26 [38400/55000 (70%)]\tLoss: 0.002328\n",
      "Train Epoch: 26 [44800/55000 (81%)]\tLoss: 0.022947\n",
      "Train Epoch: 26 [51200/55000 (93%)]\tLoss: 0.013302\n",
      "Took 11.369554996490479 seconds\n",
      "\n",
      "Test set: Average loss: 0.0672, Accuracy: 9825/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.3636, Accuracy: 7114/10000 (71%)\n",
      "\n",
      "Epoch 27/30\n",
      "Train Epoch: 27 [0/55000 (0%)]\tLoss: 0.001347\n",
      "Train Epoch: 27 [6400/55000 (12%)]\tLoss: 0.008940\n",
      "Train Epoch: 27 [12800/55000 (23%)]\tLoss: 0.002865\n",
      "Train Epoch: 27 [19200/55000 (35%)]\tLoss: 0.002039\n",
      "Train Epoch: 27 [25600/55000 (47%)]\tLoss: 0.006765\n",
      "Train Epoch: 27 [32000/55000 (58%)]\tLoss: 0.029619\n",
      "Train Epoch: 27 [38400/55000 (70%)]\tLoss: 0.010998\n",
      "Train Epoch: 27 [44800/55000 (81%)]\tLoss: 0.054143\n",
      "Train Epoch: 27 [51200/55000 (93%)]\tLoss: 0.029525\n",
      "Took 12.308922290802002 seconds\n",
      "\n",
      "Test set: Average loss: 0.0663, Accuracy: 9822/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.4110, Accuracy: 7025/10000 (70%)\n",
      "\n",
      "Epoch 28/30\n",
      "Train Epoch: 28 [0/55000 (0%)]\tLoss: 0.000688\n",
      "Train Epoch: 28 [6400/55000 (12%)]\tLoss: 0.029199\n",
      "Train Epoch: 28 [12800/55000 (23%)]\tLoss: 0.000809\n",
      "Train Epoch: 28 [19200/55000 (35%)]\tLoss: 0.001208\n",
      "Train Epoch: 28 [25600/55000 (47%)]\tLoss: 0.000328\n",
      "Train Epoch: 28 [32000/55000 (58%)]\tLoss: 0.017814\n",
      "Train Epoch: 28 [38400/55000 (70%)]\tLoss: 0.009375\n",
      "Train Epoch: 28 [44800/55000 (81%)]\tLoss: 0.001225\n",
      "Train Epoch: 28 [51200/55000 (93%)]\tLoss: 0.023555\n",
      "Took 12.394510746002197 seconds\n",
      "\n",
      "Test set: Average loss: 0.0700, Accuracy: 9808/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.4000, Accuracy: 7109/10000 (71%)\n",
      "\n",
      "Epoch 29/30\n",
      "Train Epoch: 29 [0/55000 (0%)]\tLoss: 0.011485\n",
      "Train Epoch: 29 [6400/55000 (12%)]\tLoss: 0.025447\n",
      "Train Epoch: 29 [12800/55000 (23%)]\tLoss: 0.008641\n",
      "Train Epoch: 29 [19200/55000 (35%)]\tLoss: 0.009008\n",
      "Train Epoch: 29 [25600/55000 (47%)]\tLoss: 0.035547\n",
      "Train Epoch: 29 [32000/55000 (58%)]\tLoss: 0.000449\n",
      "Train Epoch: 29 [38400/55000 (70%)]\tLoss: 0.000689\n",
      "Train Epoch: 29 [44800/55000 (81%)]\tLoss: 0.006841\n",
      "Train Epoch: 29 [51200/55000 (93%)]\tLoss: 0.004268\n",
      "Took 12.16820502281189 seconds\n",
      "\n",
      "Test set: Average loss: 0.0659, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.4218, Accuracy: 7118/10000 (71%)\n",
      "\n",
      "Epoch 30/30\n",
      "Train Epoch: 30 [0/55000 (0%)]\tLoss: 0.003002\n",
      "Train Epoch: 30 [6400/55000 (12%)]\tLoss: 0.009408\n",
      "Train Epoch: 30 [12800/55000 (23%)]\tLoss: 0.008091\n",
      "Train Epoch: 30 [19200/55000 (35%)]\tLoss: 0.031119\n",
      "Train Epoch: 30 [25600/55000 (47%)]\tLoss: 0.005795\n",
      "Train Epoch: 30 [32000/55000 (58%)]\tLoss: 0.005426\n",
      "Train Epoch: 30 [38400/55000 (70%)]\tLoss: 0.003021\n",
      "Train Epoch: 30 [44800/55000 (81%)]\tLoss: 0.000508\n",
      "Train Epoch: 30 [51200/55000 (93%)]\tLoss: 0.011171\n",
      "Took 11.357203960418701 seconds\n",
      "\n",
      "Test set: Average loss: 0.0671, Accuracy: 9828/10000 (98%)\n",
      "\n",
      "Now testing on noisy dataset...\n",
      "\n",
      "Test set: Average loss: 1.3945, Accuracy: 7164/10000 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#model = FeedforwardNet(num_hidden=500).to(device)\n",
    "model = FeedbackNet().to(device)\n",
    "print('Model has', count_parameters(model), 'trianable parameters')\n",
    "\n",
    "# TODO: Maybe use Adam or Adadelta instead. \n",
    "optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=params['momentum'])\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    print('Epoch {}/{}'.format(epoch+1, params['num_epochs']))\n",
    "    start_time = time.time()\n",
    "    train(model, device, train_loader, optimizer, epoch+1)\n",
    "    print('Took', time.time()-start_time, 'seconds')\n",
    "    test(model, device, test_loader)\n",
    "    print('Now testing on noisy dataset...')\n",
    "    test(model, device, noisy_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "feedback-connections.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
